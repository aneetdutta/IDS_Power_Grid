{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power system security Analysis\n",
    "\n",
    "### Importing Libraries\n",
    "\n",
    "Note you will need Numpy, Pandas, sklearn, matplotlib, scipy libraries to run this code so make sure they are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.linear_model import LogisticRegressionCV as LGR\n",
    "from sklearn.linear_model import LinearRegression as LNR\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up working path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Setting the Diirectory path'''\n",
    "dir_path = (os.getcwd() + \"\\\\\").replace(\"\\\\\",\"/\") # If it does not work change it to path where data is stored.\n",
    "print(\"Working directory is : \", dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_excel(dir_path + 'HW_TESLA.xls')\n",
    "data_all = data_all.fillna(0)\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation among data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[7,7])\n",
    "plt.matshow(data_all.corr(), fignum=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is highly correlated so We can reduce number of features to look at by using feature selction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Training and Test data\n",
    "\n",
    "Splitting of data has been done considering that the data domain is uniform for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr = data_all.values\n",
    "Y = data_arr[:,0]\n",
    "X = data_arr[:,1:]\n",
    "X_train, X_tt, Y_train, Y_tt = train_test_split(X, Y,stratify=Y,test_size=0.5, random_state = 15)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_tt, Y_tt,stratify=Y_tt,test_size=0.5, random_state =30)\n",
    "print('Training Data Size', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Feature selection can significantly reduce the number of observations that are needed to be taken for a given task. We are using Select K best feature selector and chi sqaure testing which selects the features which has 5 best chi square values according to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectKBest(chi2, k=5)\n",
    "sel.fit(X_train - np.min(np.min(X_train)) + 20,Y_train)\n",
    "X_train = sel.transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = sel.get_support(True)\n",
    "indices = np.concatenate((np.array([-1]),indices))\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.columns[indices+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data_all[data_all.columns[indices+1]]\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sel.transform(X_test)\n",
    "X_val = sel.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final features that are needed to be observed for classification are 'RINALDI', 'TOLUCA', 'MIGUEL', 'MIGUELMP', 'VALLEYSC' values and are sufficient for classification as we will see down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class for one part of ensemmble methods\n",
    "\n",
    "Many Classification models can over fit and gives complex decision boundariesover data but assuming that every model overfits the data in a different waywe have applied ensemble method over all the models by taking the majorityvoting of models (arbiter over all models).  This approach guarantees to reducethe over fitting of individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emodel:\n",
    "    '''\n",
    "    This is a class for a individual model in an ensemble model.\n",
    "    '''\n",
    "    # Constructor\n",
    "    def __init__(self, name, **kwargs):\n",
    "        self.em = None\n",
    "        self.name = name\n",
    "        # Check for type of model\n",
    "        '''\n",
    "        Only following models allowed you can check the meaning in import statements\n",
    "        '''\n",
    "        if(self.name == 'RFC'):\n",
    "            self.em = RFC(**kwargs)\n",
    "        elif(self.name == 'LGR'):\n",
    "            self.em = LGR(**kwargs)\n",
    "        elif(self.name == 'LNR'):\n",
    "            self.em = LNR(**kwargs)\n",
    "        elif(self.name == 'SVM'):\n",
    "            self.em = SVM(**kwargs)\n",
    "        elif(self.name == 'MLP'):\n",
    "            self.em = MLP(**kwargs)\n",
    "        elif(self.name == 'GNB'):\n",
    "            self.em = GNB(**kwargs)\n",
    "        elif(self.name == 'RNC'):\n",
    "            self.em = RNC(**kwargs)\n",
    "        elif(self.name == 'KNC'):\n",
    "            self.em = KNC(**kwargs)\n",
    "        else:\n",
    "            pass\n",
    "    def fit(self,X,Y):\n",
    "        '''\n",
    "        Training this model\n",
    "        '''\n",
    "        self.em.fit(X,Y)\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        predicting using this model\n",
    "        '''\n",
    "        return self.em.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemmble Model Class\n",
    "\n",
    "This Class ensembles all the individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier:\n",
    "    '''\n",
    "    Ensemble Model\n",
    "    \n",
    "    What does it do?\n",
    "    -------------------\n",
    "    Applies PCA on data\n",
    "    Normalizes data\n",
    "    Trains individual models\n",
    "    Predicts the classes and score\n",
    "    '''\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.models = [] #List of individual models\n",
    "        self.fr = None\n",
    "      \n",
    "    # setting presets mean and standard deviation to normalize each test point\n",
    "    def normalize_set(self,X): \n",
    "        '''\n",
    "        Setting the training data mean and std to normalize test data too\n",
    "        '''\n",
    "        self.mu = X.mean(axis=0)\n",
    "        self.std = X.std(axis=0)\n",
    "        for i in range(len(self.std)): # removing any zero valued standard deviation.\n",
    "            if(self.std[i] == 0):\n",
    "                self.std[i] = 1e-60 #If std is zero make is near to 0 to avoid 1/0 error\n",
    "    \n",
    "    # normalizing X with presets\n",
    "    def normalize(self, X):\n",
    "        '''\n",
    "        Normalize X with presetted mean and std\n",
    "        '''\n",
    "        X_norm = (X - self.mu)/(self.std)\n",
    "        return X_norm\n",
    "        \n",
    "    # Fuction to append a new model\n",
    "    def append(self, name, **kwargs):\n",
    "        '''\n",
    "        Add Models to the list of individual models.\n",
    "        '''\n",
    "        var = emodel(name, **kwargs)\n",
    "        if(var.em != None):\n",
    "            self.models.append(var)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # PCA applied for feature reduction\n",
    "    def reduce_feature(self, X, tol = 0.99):\n",
    "        '''\n",
    "        Feature reduction PCA\n",
    "        \n",
    "        X   : N x D shape data matrix\n",
    "        tol : tolerance over data information to be kept\n",
    "        '''\n",
    "        for i in range(X.shape[1]):\n",
    "            self.fr = PCA(n_components = i)\n",
    "            self.fr.fit(X)\n",
    "            tot = np.sum(self.fr.explained_variance_ratio_[0:i])\n",
    "            if(tot >= tol):\n",
    "                break;\n",
    "        self.ndims = self.fr.explained_variance_ratio_.shape[0]\n",
    "\n",
    "        X_red = self.fr.transform(X) \n",
    "        self.normalize_set(X_red)\n",
    "        \n",
    "    # Training all models\n",
    "    def train(self, X, Y, Xval, Yval):\n",
    "        '''\n",
    "        Training the models\n",
    "        '''\n",
    "        X_red = self.fr.transform(X) \n",
    "        X_red_nor = self.normalize(X_red)\n",
    "        # training each model individually\n",
    "        for model in self.models:\n",
    "            '''\n",
    "            Training each model individually\n",
    "            '''\n",
    "            model.fit(X_red_nor,Y)\n",
    "        \n",
    "        \n",
    "    # taking predictions from all the models\n",
    "    def vote(self,X):\n",
    "        '''\n",
    "        Arbiter Models\n",
    "        \n",
    "        Takes votes of all the models\n",
    "        '''\n",
    "        votes = None\n",
    "        flag = False\n",
    "        for model in self.models:\n",
    "            var = model.predict(X)\n",
    "            if(not flag):\n",
    "                votes = var.reshape(-1,1)\n",
    "                flag = True\n",
    "            else:\n",
    "                votes = np.hstack((votes,var.reshape(-1,1)))\n",
    "        \n",
    "        return votes\n",
    "\n",
    "    # To calculate accuracy\n",
    "    def score(self,X,Y):\n",
    "        '''\n",
    "        Calculates the accuracy of ensemmble model.\n",
    "        '''\n",
    "        \n",
    "        Yp = self.predict(X)\n",
    "        cper = np.sum(Yp.reshape(-1,1) == Y.reshape(-1,1))/Y.shape[0]\n",
    "        return cper\n",
    "    \n",
    "    # To predict class\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts the class array of a given input array\n",
    "        '''\n",
    "        \n",
    "        X_red = self.fr.transform(X)\n",
    "        Xt = self.normalize(X_red)\n",
    "        \n",
    "        votes = self.vote(Xt)\n",
    "        Y = np.squeeze(stats.mode(votes,axis=1)[0])\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier() # Creating a new ensemble classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reduce_feature(X_train,tol=0.999999)\n",
    "ndims = model.fr.explained_variance_ratio_.shape[0]\n",
    "X_red_nor = model.fr.transform(X_train)\n",
    "print('Features retained = ', ndims)\n",
    "X_red = model.fr.transform(X_train)\n",
    "X_red_nor = model.normalize(X_red)\n",
    "# print(X_norm.shape, Y_train.shape, X_red_nor.shape)\n",
    "fig, ax = plt.subplots(ndims,ndims,figsize =[10,10])\n",
    "for i in range(ndims):\n",
    "    for j in range(ndims):\n",
    "        ax[i][j].scatter(X_red_nor[:,i],X_red_nor[:,j],c = Y_train, cmap=\"jet\", marker ='.', alpha = 0.1)\n",
    "plt.savefig('scatterplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = model.fr.explained_variance_ratio_\n",
    "sums = [np.sum(lams[0:i]) for i in range(lams.shape[0]+1)]\n",
    "plt.plot([1,2,3,4,5],sums)\n",
    "plt.plot([1,lams.shape[0]+1],[1,1],'b--')\n",
    "plt.xlabel(\"Numbers of retained eigen vectors\", fontsize=\"14\")\n",
    "plt.ylabel(\"Data Information kept\", fontsize=\"14\")\n",
    "plt.savefig(\"003.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can just add and remove any number of models you like but they all should follow similar fit and predict function implementation in sklearn.\n",
    "\n",
    "Example:\n",
    "\n",
    "You want only one ensemmble Logistic regression then comment out everything else and keep logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adding the individual models to the ensemble model.\n",
    "\n",
    "we tried different coombinations thats why some models are commented out\n",
    "'''\n",
    "model.models = [] # lsit of models (empty intially)\n",
    "model.append('LGR', random_state=1) # logistic regression\n",
    "model.append('RFC', random_state=1) # random forest\n",
    "# model.append('KNC') # K nearest neighbours\n",
    "# model.append('SVM', kernel = 'rbf', random_state=1) # Support vector radial basis function kernel\n",
    "# model.append('SVM', kernel = 'linear', random_state=1) # support vector linear kernel\n",
    "# model.append('SVM', kernel = 'sigmoid', random_state=1) # support vector linear kernel\n",
    "model.append('SVM', kernel = 'poly', degree = 5, random_state=1) # support vecotr polynomial kernel degree = 3\n",
    "model.append('SVM', kernel = 'poly', degree = 8, random_state=1) # support vecotr polynomial kernel degree = 3\n",
    "# model.append('SVM', kernel = 'poly', degree = 12, random_state=1) # support vecotr polynomial kernel degree = 3\n",
    "#### MLP mean muti layer perceptron (Artificial Nueral Networks) ###\n",
    "model.append('MLP', solver='lbfgs', alpha=1, hidden_layer_sizes=(25,25,2), max_iter = 10000, random_state = 12)\n",
    "model.append('MLP', solver='lbfgs', alpha=1, hidden_layer_sizes=(5,5,2), max_iter = 10000, random_state = 10)\n",
    "model.append('MLP', solver='lbfgs', alpha=2, hidden_layer_sizes=(10,2), max_iter = 10000, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_train, Y_train, X_val, Y_val) # training the ensembel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculating the Accuracies\n",
    "'''\n",
    "var_test = model.score(X_test,Y_test)\n",
    "var_val = model.score(X_val,Y_val)\n",
    "var_train = model.score(X_train,Y_train)\n",
    "print('Test Accuracy:',var_test,'\\nVal Accuracy:',var_val,'\\nTrain Accuracy:',var_train,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_preds_test = model.predict(X_test)\n",
    "Y_preds_train = model.predict(X_train)\n",
    "Y_preds_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test data confusion matrix\\n--------------------------------\")\n",
    "print(confusion_matrix(Y_test, Y_preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data confusion matrix\\n--------------------------------\")\n",
    "print(confusion_matrix(Y_train, Y_preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation data confusion matrix\\n--------------------------------\")\n",
    "print(confusion_matrix(Y_val, Y_preds_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "FP_train = 0, FN_train = 0\n",
    "\n",
    "FP_test = 1, FN_test = 2\n",
    "\n",
    "FP_val = 1, FN_val = 2\n",
    "\n",
    "Note these values may differ on different train test splits but wont differ alot with respect to data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thankyou "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
